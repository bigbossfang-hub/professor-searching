import streamlit as st
import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
import time
import urllib.parse
import re
import json
import xml.etree.ElementTree as ET
import google.generativeai as genai
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê°•ì‚¬ ê³ ê¸‰ ê²€ìƒ‰",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ì»¤ìŠ¤í…€ CSS - ì°¨ë¶„í•˜ê³  ì„¸ë ¨ëœ ìƒ‰ìƒ í…Œë§ˆ
st.markdown("""
    <style>
    .stApp {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    }
    
    .main-title {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
    }
    
    .main-title h1 {
        color: white !important;
        margin: 0 !important;
    }
    
    .main-title p {
        color: rgba(255, 255, 255, 0.95) !important;
        margin-top: 0.5rem !important;
    }
    
    .instructor-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
        margin: 1rem 0;
        border-left: 5px solid #667eea;
    }
    
    .instructor-card.selected {
        border-left-color: #f093fb;
        background: #faf0ff;
    }
    
    .info-card {
        background: white;
        padding: 2rem;
        border-radius: 10px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
        margin: 1rem 0;
        border-left: 6px solid #764ba2;
    }
    
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.75rem 2rem;
        font-weight: 600;
    }
    
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #667eea 0%, #764ba2 100%);
    }
    
    [data-testid="stSidebar"] h1,
    [data-testid="stSidebar"] h2,
    [data-testid="stSidebar"] h3 {
        color: white !important;
    }
    </style>
""", unsafe_allow_html=True)

# Google Sheets ì—°ê²° í•¨ìˆ˜
@st.cache_resource
def get_google_sheet():
    """êµ¬ê¸€ ì‹œíŠ¸ì— ì—°ê²°"""
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        SERVICE_ACCOUNT_FILE = os.path.join(current_dir, 'huhsame-service-account-key.json')
        
        SCOPES = [
            'https://www.googleapis.com/auth/spreadsheets.readonly',
            'https://www.googleapis.com/auth/drive.readonly'
        ]
        
        credentials = Credentials.from_service_account_file(
            SERVICE_ACCOUNT_FILE,
            scopes=SCOPES
        )
        
        client = gspread.authorize(credentials)
        SPREADSHEET_ID = '1-EaykQMr06Qm9FWDOJX3CVbAZylGom1G'
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.get_worksheet(0)
        
        return worksheet
    except FileNotFoundError as e:
        return None
    except Exception as e:
        return None

@st.cache_data(ttl=3600)
def load_instructor_data():
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ê°•ì‚¬ ë°ì´í„° ë¡œë“œ"""
    # ë°©ë²• 1: CSV URLë¡œ ê³µê°œ ë°ì´í„° ì ‘ê·¼ ì‹œë„
    try:
        SPREADSHEET_ID = '1-EaykQMr06Qm9FWDOJX3CVbAZylGom1G'
        csv_url = f'https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=csv&gid=0'
        df = pd.read_csv(csv_url)
        
        if not df.empty:
            df.columns = df.columns.str.strip()
            return df
    except:
        pass
    
    # ë°©ë²• 2: ì„œë¹„ìŠ¤ ê³„ì •ì„ í†µí•œ gspread ì‚¬ìš©
    try:
        worksheet = get_google_sheet()
        if worksheet is not None:
            all_values = worksheet.get_all_values()
            if len(all_values) > 1:
                df = pd.DataFrame(all_values[1:], columns=all_values[0])
                if not df.empty:
                    df.columns = df.columns.str.strip()
                    return df
    except:
        pass
    
    # ë°©ë²• 3: CSV URL without gid íŒŒë¼ë¯¸í„° ì‹œë„
    try:
        SPREADSHEET_ID = '1-EaykQMr06Qm9FWDOJX3CVbAZylGom1G'
        csv_url = f'https://docs.google.com/spreadsheets/d/{SPREADSHEET_ID}/export?format=csv'
        df = pd.read_csv(csv_url)
        
        if not df.empty:
            df.columns = df.columns.str.strip()
            return df
    except:
        pass
    
    return pd.DataFrame()

def search_instructors(df, query, search_type='all'):
    """
    ê°•ì‚¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    search_type: 'name' (ê°•ì‚¬ì´ë¦„), 'field' (ëŒ€ë¶„ì•¼/ì†Œë¶„ì•¼), 'subject' (ê°•ì˜ ê³¼ëª©), 'all' (ì „ì²´)
    """
    if df.empty or not query:
        return pd.DataFrame()
    
    # ê²€ìƒ‰í•  ì»¬ëŸ¼ ì°¾ê¸°
    name_cols = [col for col in df.columns if 'ê°•ì‚¬' in col and 'ì´ë¦„' in col]
    field_cols = [col for col in df.columns if any(x in col for x in ['ëŒ€ë¶„ì•¼', 'ì†Œë¶„ì•¼', 'ë¶„ì•¼'])]
    subject_cols = [col for col in df.columns if 'ê°•ì˜' in col and 'ê³¼ëª©' in col]
    
    # ê²°ê³¼ ì €ì¥
    results = pd.DataFrame()
    
    # ê²€ìƒ‰ íƒ€ì…ì— ë”°ë¼ í•„í„°ë§
    if search_type == 'all' or search_type == 'name':
        for col in name_cols:
            mask = df[col].astype(str).str.contains(query, case=False, na=False)
            results = pd.concat([results, df[mask]], ignore_index=True)
    
    if search_type == 'all' or search_type == 'field':
        for col in field_cols:
            mask = df[col].astype(str).str.contains(query, case=False, na=False)
            results = pd.concat([results, df[mask]], ignore_index=True)
    
    if search_type == 'all' or search_type == 'subject':
        for col in subject_cols:
            mask = df[col].astype(str).str.contains(query, case=False, na=False)
            results = pd.concat([results, df[mask]], ignore_index=True)
    
    # ì¤‘ë³µ ì œê±° - ì´ë¦„ê³¼ ì´ë©”ì¼ ì£¼ì†Œê°€ ê°™ì€ ê²½ìš° ë™ì¼ì¸ë¬¼ë¡œ íŒë‹¨
    if not results.empty:
        # ì´ë¦„ê³¼ ì´ë©”ì¼ ì»¬ëŸ¼ ì°¾ê¸° (results DataFrame ê¸°ì¤€)
        result_name_cols = [col for col in results.columns if 'ê°•ì‚¬' in col and 'ì´ë¦„' in col]
        email_cols = [col for col in results.columns if 'e-mail' in col or 'ì´ë©”ì¼' in col]
        
        if result_name_cols and email_cols:
            # ì´ë¦„ê³¼ ì´ë©”ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            # ì²« ë²ˆì§¸ ê¸°ì¤€: ì´ë¦„ + ì´ë©”ì¼ì´ ëª¨ë‘ ê°™ì€ ê²½ìš°
            name_col = result_name_cols[0]
            email_col = email_cols[0]
            
            # ì´ë¦„ê³¼ ì´ë©”ì¼ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¤‘ë³µ ì œê±°
            results = results.drop_duplicates(subset=[name_col, email_col], keep='first')
        else:
            # ì´ë¦„ì´ë‚˜ ì´ë©”ì¼ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ì¤‘ë³µ ì œê±°
            results = results.drop_duplicates()
    
    return results

def search_naver_person(person_name):
    """
    ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰ì—ì„œ ê°•ì‚¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        # ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰ URL
        encoded_name = urllib.parse.quote(person_name)
        url = f"https://search.naver.com/search.naver?where=nexearch&query={encoded_name}"
        
        # í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        # ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        result = {
            'name': person_name,
            'source': 'ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰',
            'url': url,
            'info': {}
        }
        
        # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì¸ë¬¼ ì •ë³´ ì°¾ê¸°
        person_card = None
        
        # ë°©ë²• 1: people_info í´ë˜ìŠ¤
        person_card = soup.find('div', class_='people_info')
        
        # ë°©ë²• 2: api_subject_bx í´ë˜ìŠ¤
        if not person_card:
            person_card = soup.find('div', class_='api_subject_bx')
        
        # ë°©ë²• 3: api_ani_send í´ë˜ìŠ¤
        if not person_card:
            person_card = soup.find('div', class_='api_ani_send')
        
        # ë°©ë²• 4: ì¸ë¬¼ ì •ë³´ê°€ í¬í•¨ëœ ì„¹ì…˜ ì°¾ê¸°
        if not person_card:
            sections = soup.find_all('section', class_=lambda x: x and 'people' in x.lower())
            if sections:
                person_card = sections[0]
        
        if person_card:
            # ì œëª©/ì´ë¦„ ì¶”ì¶œ
            title = person_card.find('h2', class_='title') or person_card.find('h2')
            if not title:
                title = person_card.find('h3', class_='title') or person_card.find('h3')
            if title:
                title_text = title.get_text(strip=True)
                if title_text:
                    result['info']['ì´ë¦„'] = title_text
            
            # ì •ë³´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (dt/dd êµ¬ì¡°)
            info_list = person_card.find('ul', class_='lst_total') or person_card.find('ul')
            if info_list:
                items = info_list.find_all('li')
                for item in items:
                    dt = item.find('dt')
                    dd = item.find('dd')
                    if dt and dd:
                        key = dt.get_text(strip=True).replace(':', '').strip()
                        value = dd.get_text(strip=True)
                        if key and value:
                            result['info'][key] = value
                    else:
                        # dt/ddê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ í‚¤-ê°’ ì¶”ì¶œ ì‹œë„
                        text = item.get_text(strip=True)
                        if ':' in text:
                            parts = text.split(':', 1)
                            if len(parts) == 2:
                                key = parts[0].strip()
                                value = parts[1].strip()
                                if key and value:
                                    result['info'][key] = value
            
            # ì„¤ëª… ì •ë³´ ì¶”ì¶œ
            desc = person_card.find('div', class_='dsc') or person_card.find('p', class_='dsc')
            if desc:
                desc_text = desc.get_text(strip=True)
                if desc_text and len(desc_text) > 10:  # ì˜ë¯¸ìˆëŠ” ì„¤ëª…ë§Œ
                    result['info']['ì„¤ëª…'] = desc_text
        
        # ì¶”ê°€ ì •ë³´: ë°”ì´ì˜¤ê·¸ë˜í”¼ ì„¹ì…˜
        bio_section = soup.find('section', class_='api_biography') or soup.find('div', class_='api_biography')
        if bio_section:
            bio_items = bio_section.find_all('li')
            biographies = []
            for item in bio_items:
                bio_text = item.get_text(strip=True)
                if bio_text and len(bio_text) > 5:
                    biographies.append(bio_text)
            if biographies:
                result['info']['ì•½ë ¥'] = ' | '.join(biographies[:5])  # ìµœëŒ€ 5ê°œë§Œ
        
        # í”„ë¡œí•„ ì´ë¯¸ì§€ ì°¾ê¸°
        img = soup.find('img', class_='thumb') or soup.find('img', class_='_img')
        if img:
            img_src = img.get('src') or img.get('data-src')
            if img_src:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if img_src.startswith('//'):
                    img_src = 'https:' + img_src
                elif img_src.startswith('/'):
                    img_src = 'https://search.naver.com' + img_src
                result['info']['ì´ë¯¸ì§€'] = img_src
        
        # ìœ íŠœë¸Œ ë§í¬ëŠ” ë„¤ì´ë²„ì—ì„œ ì°¾ì§€ ì•Šê³  ì§ì ‘ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‚¬ìš©
        # (ì—¬ëŸ¬ ë§í¬ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” search_youtube_channel í•¨ìˆ˜ ì‚¬ìš©)
        # result['info']['ìœ íŠœë¸Œ']ëŠ” ì €ì¥í•˜ì§€ ì•Šê³ , ë‚˜ì¤‘ì— display ë‹¨ê³„ì—ì„œ search_youtube_channel í˜¸ì¶œ
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(result['info']) == 0:
            # ê°„ë‹¨í•œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
            no_result = soup.find('div', class_='_empty_state')
            if no_result or 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤' in response.text[:5000]:
                return None
            # ì •ë³´ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜í•˜ì§€ ì•Šê³  ë¹ˆ ì •ë³´ë¼ë„ ë°˜í™˜
            return result
        
        return result
        
    except requests.exceptions.RequestException as e:
        # ì—ëŸ¬ë¥¼ í‘œì‹œí•˜ì§€ ì•Šê³  None ë°˜í™˜ (ì¡°ìš©íˆ ì‹¤íŒ¨)
        return None
    except Exception as e:
        # ì—ëŸ¬ë¥¼ í‘œì‹œí•˜ì§€ ì•Šê³  None ë°˜í™˜ (ì¡°ìš©íˆ ì‹¤íŒ¨)
        return None

def filter_relevant_youtube_links(links, person_name, job=None, main_field=None, sub_field=None):
    """
    ìœ íŠœë¸Œ ë§í¬ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ì—¬ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        links: ìœ íŠœë¸Œ ë§í¬ ë¦¬ìŠ¤íŠ¸
        person_name: ê°•ì‚¬ ì´ë¦„
        job: ì§ì—…
        main_field: ëŒ€ë¶„ì•¼
        sub_field: ì†Œë¶„ì•¼
        
    Returns:
        ê´€ë ¨ì„±ì´ ë†’ì€ ë§í¬ë§Œ í•„í„°ë§í•œ ë¦¬ìŠ¤íŠ¸
    """
    if not links:
        return []
    
    filtered = []
    
    for link in links:
        # ê²€ìƒ‰ URLì´ë‚˜ ì±„ë„ì€ ì¼ë‹¨ í¬í•¨
        if link.get('type') in ['search', 'channel']:
            filtered.append(link)
            continue
        
        title = link.get('title', '').lower()
        score = 0
        
        # 1. ê°•ì‚¬ ì´ë¦„ì´ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ê°€ì¥ ì¤‘ìš”)
        name_parts = person_name.split()
        name_found = False
        for name_part in name_parts:
            if len(name_part) >= 2 and name_part.lower() in title:
                score += 3
                name_found = True
                break
        
        # ì´ë¦„ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜ ë‚®ìŒ
        if not name_found:
            score -= 2
        
        # 2. ì§ì—… í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if job:
            job_keywords = ['êµìˆ˜', 'ë°•ì‚¬', 'ê°•ì‚¬', 'ceo', 'ëŒ€í‘œ', 'ì´ì‚¬', 'ì—°êµ¬ì›', 'êµì‚¬', 'ì „ë¬¸ê°€', 'ì»¨ì„¤í„´íŠ¸']
            for keyword in job_keywords:
                if keyword in str(job).lower() and keyword in title:
                    score += 2
                    break
        
        # 3. ì†Œë¶„ì•¼ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if sub_field:
            field_parts = str(sub_field).lower().split()
            for part in field_parts:
                if len(part) >= 2 and part in title:
                    score += 2
                    break
        
        # 4. ëŒ€ë¶„ì•¼ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if main_field and not sub_field:
            field_parts = str(main_field).lower().split()
            for part in field_parts:
                if len(part) >= 2 and part in title:
                    score += 1
                    break
        
        # 5. ê°•ì˜/ê°•ì—°/ì„¸ë¯¸ë‚˜ ë“± êµìœ¡ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°€ì‚°ì 
        education_keywords = ['ê°•ì˜', 'ê°•ì—°', 'ì„¸ë¯¸ë‚˜', 'íŠ¹ê°•', 'ê°•ì¢Œ', 'êµìœ¡', 'lecture', 'seminar']
        for keyword in education_keywords:
            if keyword in title:
                score += 1
                break
        
        # 6. ê´€ë ¨ ì—†ëŠ” í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°ì 
        irrelevant_keywords = ['ë¨¹ë°©', 'ì¼ìƒ', 'vlog', 'ë¸Œì´ë¡œê·¸', 'ì—¬í–‰', 'ë§›ì§‘', 'ê²Œì„', 'ë¦¬ë·°', 'ì–¸ë°•ì‹±']
        for keyword in irrelevant_keywords:
            if keyword in title:
                score -= 3
                break
        
        # ì ìˆ˜ê°€ ì¼ì • ê¸°ì¤€ ì´ìƒì¸ ë§í¬ë§Œ í¬í•¨ (ì´ë¦„ì´ ìˆê±°ë‚˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ ì¶©ë¶„í•œ ê²½ìš°)
        if score >= 1:
            link['relevance_score'] = score
            filtered.append(link)
    
    # ì›ë˜ ìˆœì„œ ìœ ì§€ (ìµœì‹ ìˆœ = ê²€ìƒ‰ ê²°ê³¼ ìˆœì„œ)
    # ì ìˆ˜ëŠ” í•„í„°ë§ ê¸°ì¤€ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì •ë ¬ì€ ì›ë˜ ìˆœì„œëŒ€ë¡œ
    filtered.sort(key=lambda x: x.get('order', 0))
    
    return filtered

def search_youtube_channel(person_name, job=None, main_field=None, sub_field=None):
    """
    ìœ íŠœë¸Œì—ì„œ ì¸ë¬¼ì˜ ì±„ë„/ë™ì˜ìƒì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    ì—¬ëŸ¬ ìœ íŠœë¸Œ ë§í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤ (ìµœì‹ ìˆœ).
    
    Args:
        person_name: ê²€ìƒ‰í•  ì¸ë¬¼ ì´ë¦„
        job: ì§ì—… (ì˜ˆ: êµìˆ˜, ê°•ì‚¬, CEO ë“±)
        main_field: ëŒ€ë¶„ì•¼ (ì˜ˆ: ê²½ì˜, ë§ˆì¼€íŒ… ë“±)
        sub_field: ì†Œë¶„ì•¼ (ì˜ˆ: ë””ì§€í„¸ë§ˆì¼€íŒ…, ì „ëµê²½ì˜ ë“±)
    """
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê´€ë ¨ ì •ë³´ í¬í•¨)
        search_query_parts = [person_name]
        
        # ì§ì—… ì¶”ê°€ (êµìˆ˜, ê°•ì‚¬ ë“± ì‹ ë¢°ë„ ë†’ì€ ì •ë³´)
        if job and pd.notna(job) and job.strip():
            # ì§ì—… ì¤‘ ì¤‘ìš”í•œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
            job_keywords = ['êµìˆ˜', 'ë°•ì‚¬', 'ê°•ì‚¬', 'CEO', 'ëŒ€í‘œ', 'ì´ì‚¬', 'ì—°êµ¬ì›', 'êµì‚¬', 'ì „ë¬¸ê°€']
            for keyword in job_keywords:
                if keyword in str(job):
                    search_query_parts.append(keyword)
                    break
        
        # ì†Œë¶„ì•¼ ìš°ì„  (ë” êµ¬ì²´ì )
        if sub_field and pd.notna(sub_field) and sub_field.strip():
            # ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¶”ê°€ (3ë‹¨ì–´ ì´í•˜)
            if len(str(sub_field).split()) <= 3:
                search_query_parts.append(str(sub_field))
        # ëŒ€ë¶„ì•¼ ì¶”ê°€ (ì†Œë¶„ì•¼ê°€ ì—†ëŠ” ê²½ìš°)
        elif main_field and pd.notna(main_field) and main_field.strip():
            if len(str(main_field).split()) <= 2:
                search_query_parts.append(str(main_field))
        
        # ìµœì¢… ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        search_query = ' '.join(search_query_parts)
        encoded_name = urllib.parse.quote(search_query)
        search_url = f"https://www.youtube.com/results?search_query={encoded_name}"
        
        # ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œë„
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        page_text = response.text
        
        youtube_links = []
        seen_ids = set()
        
        # ë°©ë²• 1: ytInitialDataì—ì„œ JSON íŒŒì‹±
        try:
            # ytInitialData ì°¾ê¸°
            if 'var ytInitialData = ' in page_text:
                start = page_text.find('var ytInitialData = ') + len('var ytInitialData = ')
                end = page_text.find(';</script>', start)
                if end > start:
                    json_str = page_text[start:end]
                    try:
                        data = json.loads(json_str)
                        
                        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë™ì˜ìƒ ì¶”ì¶œ
                        contents = data.get('contents', {}).get('twoColumnSearchResultsRenderer', {}).get('primaryContents', {}).get('sectionListRenderer', {}).get('contents', [])
                        
                        for content in contents:
                            item_section = content.get('itemSectionRenderer', {})
                            for item in item_section.get('contents', []):
                                # ë™ì˜ìƒ ì•„ì´í…œ
                                if 'videoRenderer' in item:
                                    video = item['videoRenderer']
                                    video_id = video.get('videoId')
                                    if video_id and len(video_id) == 11 and video_id not in seen_ids:
                                        title = video.get('title', {}).get('runs', [{}])[0].get('text', f'ë™ì˜ìƒ {len(youtube_links) + 1}')
                                        
                                        # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                                        published_time = None
                                        if 'publishedTimeText' in video:
                                            published_time = video['publishedTimeText'].get('simpleText', '')
                                        elif 'publishedTime' in video:
                                            published_time = video['publishedTime']
                                        
                                        youtube_links.append({
                                            'type': 'video',
                                            'url': f"https://www.youtube.com/watch?v={video_id}",
                                            'id': video_id,
                                            'title': title,
                                            'published': published_time,
                                            'order': len(youtube_links)  # ìˆœì„œ ì €ì¥ (ìµœì‹ ìˆœ)
                                        })
                                        seen_ids.add(video_id)
                                
                                # ì±„ë„ ì•„ì´í…œ
                                elif 'channelRenderer' in item:
                                    channel = item['channelRenderer']
                                    channel_id = channel.get('channelId')
                                    if channel_id and channel_id not in seen_ids:
                                        title = channel.get('title', {}).get('simpleText', f'ì±„ë„ {len([x for x in youtube_links if x["type"] == "channel"]) + 1}')
                                        youtube_links.append({
                                            'type': 'channel',
                                            'url': f"https://www.youtube.com/channel/{channel_id}",
                                            'id': channel_id,
                                            'title': title,
                                            'published': None,  # ì±„ë„ì€ ë‚ ì§œ ì •ë³´ ì—†ìŒ
                                            'order': len(youtube_links)
                                        })
                                        seen_ids.add(channel_id)
                                
                                # ìµœëŒ€ 20ê°œê¹Œì§€
                                if len(youtube_links) >= 20:
                                    break
                            
                            if len(youtube_links) >= 20:
                                break
                    except json.JSONDecodeError:
                        pass
        except Exception as e:
            pass
        
        # ë°©ë²• 2: ì •ê·œì‹ìœ¼ë¡œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ (ë°±ì—…)
        if len(youtube_links) < 5:
            video_patterns = [
                r'"videoId":"([a-zA-Z0-9_-]{11})"',
                r'/watch\?v=([a-zA-Z0-9_-]{11})',
                r'watch\?v=([a-zA-Z0-9_-]{11})',
            ]
            
            for pattern in video_patterns:
                matches = re.findall(pattern, page_text)
                for video_id in matches:
                    if len(video_id) == 11 and video_id not in seen_ids:
                        youtube_links.append({
                            'type': 'video',
                            'url': f"https://www.youtube.com/watch?v={video_id}",
                            'id': video_id,
                            'title': f"ë™ì˜ìƒ {len([x for x in youtube_links if x['type'] == 'video']) + 1}",
                            'published': None,  # ë°±ì—… ë°©ë²•ì€ ë‚ ì§œ ì •ë³´ ì—†ìŒ
                            'order': len(youtube_links)
                        })
                        seen_ids.add(video_id)
                        
                        if len(youtube_links) >= 20:
                            break
                
                if len(youtube_links) >= 20:
                    break
            
            # ì±„ë„ íŒ¨í„´ë„ ì¶”ê°€
            channel_patterns = [
                r'"channelId":"([^"]+)"',
                r'/channel/([^"/\s]+)',
            ]
            
            for pattern in channel_patterns:
                matches = re.findall(pattern, page_text)
                for channel_id in matches:
                    if channel_id and len(channel_id) > 10 and channel_id not in seen_ids:
                        youtube_links.append({
                            'type': 'channel',
                            'url': f"https://www.youtube.com/channel/{channel_id}",
                            'id': channel_id,
                            'title': f"ì±„ë„ {len([x for x in youtube_links if x['type'] == 'channel']) + 1}",
                            'published': None,  # ì±„ë„ì€ ë‚ ì§œ ì •ë³´ ì—†ìŒ
                            'order': len(youtube_links)
                        })
                        seen_ids.add(channel_id)
                        
                        if len(youtube_links) >= 20:
                            break
                
                if len(youtube_links) >= 20:
                    break
        
        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€ (ìµœì‹ ìˆœ = ê²€ìƒ‰ ê²°ê³¼ ìˆœì„œ)
        unique_links = []
        seen_urls = set()
        for link in youtube_links:
            if link['url'] not in seen_urls:
                unique_links.append(link)
                seen_urls.add(link['url'])
        
        # ê´€ë ¨ì„± í•„í„°ë§ (ê°•ì˜dataì— ìˆëŠ” ê°•ì‚¬ì˜ ê²½ìš°)
        if job or main_field or sub_field:
            filtered_links = filter_relevant_youtube_links(
                unique_links, 
                person_name, 
                job, 
                main_field, 
                sub_field
            )
            
            # ê´€ë ¨ì„± ìˆëŠ” ë§í¬ê°€ ì¶©ë¶„íˆ ìˆëŠ” ê²½ìš°ë§Œ ë°˜í™˜ (ìµœì†Œ 2ê°œ ì´ìƒ)
            if len(filtered_links) >= 2:
                return filtered_links[:15]
            else:
                # ê´€ë ¨ì„± ìˆëŠ” ë§í¬ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                return []
        
        # ì¼ë°˜ ê²€ìƒ‰ (ë„¤ì´ë²„, ì§ì ‘ ê²€ìƒ‰)ì˜ ê²½ìš° í•„í„°ë§ ì—†ì´ ë°˜í™˜
        if unique_links:
            return unique_links[:15]
        
        # ì°¾ì§€ ëª»í•œ ê²½ìš° ê²€ìƒ‰ URLì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜
        return [{
            'type': 'search',
            'url': search_url,
            'id': 'search',
            'title': 'ìœ íŠœë¸Œì—ì„œ ê²€ìƒ‰',
            'published': None,
            'order': 0
        }]
        
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ê²€ìƒ‰ URL ë°˜í™˜
        encoded_name = urllib.parse.quote(person_name)
        return [{
            'type': 'search',
            'url': f"https://www.youtube.com/results?search_query={encoded_name}",
            'id': 'search',
            'title': 'ìœ íŠœë¸Œì—ì„œ ê²€ìƒ‰',
            'published': None,
            'order': 0
        }]

def extract_video_id_from_url(youtube_url):
    """
    ìœ íŠœë¸Œ URLì—ì„œ ë¹„ë””ì˜¤ IDë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    if not youtube_url:
        return None
    
    # ë‹¤ì–‘í•œ ìœ íŠœë¸Œ URL í˜•ì‹ ì²˜ë¦¬
    patterns = [
        r'youtube\.com/watch\?v=([^&]+)',  # watch?v= í˜•ì‹ (ìš°ì„ )
        r'youtu\.be/([^?]+)',  # youtu.be í˜•ì‹
        r'youtube\.com/embed/([^?]+)',  # embed í˜•ì‹
        r'(?:v=|/)([0-9A-Za-z_-]{11})',  # ì¼ë°˜ ë¹„ë””ì˜¤ ID
    ]
    
    for pattern in patterns:
        match = re.search(pattern, youtube_url)
        if match:
            video_id = match.group(1)
            # ë¹„ë””ì˜¤ IDê°€ 11ìì¸ì§€ í™•ì¸
            if len(video_id) == 11:
                return video_id
    
    return None

def get_latest_video_from_channel(channel_url):
    """
    ì±„ë„ URLì—ì„œ ìµœì‹  ë™ì˜ìƒì˜ ë¹„ë””ì˜¤ IDë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
    """
    if not channel_url:
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        # ì±„ë„ì˜ /videos í˜ì´ì§€ë¡œ ì´ë™
        if '/videos' not in channel_url:
            if channel_url.endswith('/'):
                videos_url = channel_url + 'videos'
            else:
                videos_url = channel_url + '/videos'
        else:
            videos_url = channel_url
        
        response = requests.get(videos_url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë¹„ë””ì˜¤ ID ì°¾ê¸°
        page_text = response.text
        soup = BeautifulSoup(page_text, 'html.parser')
        
        # ë°©ë²• 1: JSON ë°ì´í„°ì—ì„œ videoId ì°¾ê¸°
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                script_content = script.string
                
                # íŒ¨í„´ 1: "videoId":"xxxxx" í˜•ì‹
                video_id_patterns = [
                    r'"videoId":"([a-zA-Z0-9_-]{11})"',
                    r'"videoId":\s*"([a-zA-Z0-9_-]{11})"',
                    r'videoId["\s]*[:=]["\s]*([a-zA-Z0-9_-]{11})',
                ]
                
                for pattern in video_id_patterns:
                    matches = re.findall(pattern, script_content)
                    if matches:
                        # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ID ë°˜í™˜ (ë³´í†µ ìµœì‹  ë™ì˜ìƒ)
                        video_id = matches[0]
                        if len(video_id) == 11:
                            return video_id
                
                # íŒ¨í„´ 2: /watch?v=xxxxx í˜•ì‹
                watch_patterns = [
                    r'/watch\?v=([a-zA-Z0-9_-]{11})',
                    r'youtube\.com/watch\?v=([a-zA-Z0-9_-]{11})',
                    r'youtu\.be/([a-zA-Z0-9_-]{11})',
                ]
                
                for pattern in watch_patterns:
                    matches = re.findall(pattern, script_content)
                    if matches:
                        video_id = matches[0]
                        if len(video_id) == 11:
                            return video_id
        
        # ë°©ë²• 2: í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
        video_id_pattern = r'"videoId":"([a-zA-Z0-9_-]{11})"'
        all_matches = re.findall(video_id_pattern, page_text)
        if all_matches:
            # ì¤‘ë³µ ì œê±°í•˜ê³  ì²« ë²ˆì§¸ ë°˜í™˜
            unique_ids = list(dict.fromkeys(all_matches))
            for vid_id in unique_ids:
                if len(vid_id) == 11:
                    return vid_id
        
        return None
        
    except Exception as e:
        return None

def get_youtube_transcript(video_id, lang='ko'):
    """
    ìœ íŠœë¸Œ ë¹„ë””ì˜¤ì˜ ìë§‰/ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (youtube-transcript-api v1.2.3 ì‚¬ìš©)
    """
    if not video_id or len(video_id) != 11:
        return None
    
    try:
        # ë°©ë²• 1: youtube-transcript-api ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (v1.2.3 ë°©ì‹)
        try:
            # YouTubeTranscriptApi ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            ytt_api = YouTubeTranscriptApi()
            
            # í•œêµ­ì–´ ìë§‰ ì‹œë„
            fetched_transcript = ytt_api.fetch(video_id, languages=['ko', 'ko-KR'])
            if fetched_transcript:
                # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•´ì„œ í•©ì¹˜ê¸° (FetchedTranscript ê°ì²´ëŠ” iterable)
                transcript_text = ' '.join([snippet.text for snippet in fetched_transcript])
                if len(transcript_text.strip()) > 50:
                    return transcript_text
        except (TranscriptsDisabled, NoTranscriptFound):
            # í•œêµ­ì–´ ìë§‰ì´ ì—†ìœ¼ë©´ ì˜ì–´ ì‹œë„
            try:
                ytt_api = YouTubeTranscriptApi()
                fetched_transcript = ytt_api.fetch(video_id, languages=['en', 'en-US'])
                if fetched_transcript:
                    transcript_text = ' '.join([snippet.text for snippet in fetched_transcript])
                    if len(transcript_text.strip()) > 50:
                        return transcript_text
            except:
                pass
        except Exception as e:
            pass
        
        # ë°©ë²• 2: ì§ì ‘ ìë§‰ API í˜¸ì¶œ (ë°±ì—…)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/xml,application/xml,*/*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        captions_url = f"https://www.youtube.com/api/timedtext?v={video_id}&lang={lang}"
        response = requests.get(captions_url, headers=headers, timeout=15)
        
        if response.status_code == 200 and response.content:
            try:
                root = ET.fromstring(response.content)
                transcript_text = []
                for text_element in root.iter('text'):
                    text = text_element.text
                    if text:
                        transcript_text.append(text.strip())
                
                result = ' '.join(transcript_text)
                if len(result.strip()) > 50:
                    return result
            except:
                pass
        
        # ì˜ì–´ ìë§‰ ì‹œë„ (ë°±ì—…)
        if lang == 'ko':
            return get_youtube_transcript(video_id, 'en')
        
        return None
        
    except Exception as e:
        return None

def summarize_transcript_with_gemini(transcript, max_length=1000):
    """
    Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰/ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (1000ì ë‚´ì™¸, ëª©ì°¨ë³„ ì •ë¦¬)
    """
    if not transcript:
        return None
    
    # Gemini API í‚¤ ê°€ì ¸ì˜¤ê¸°
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        try:
            gemini_api_key = st.secrets['GEMINI_API_KEY']
        except:
            gemini_api_key = None
    
    if not gemini_api_key:
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ìš”ì•½ ë°©ë²• ì‚¬ìš©
        return summarize_transcript_fallback(transcript, max_length)
    
    try:
        # Gemini API ì´ˆê¸°í™”
        genai.configure(api_key=gemini_api_key)
        
        # ëª¨ë¸ ì„ íƒ (gemini-2.0-flash ìš°ì„ , ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ëª¨ë¸ ì‹œë„)
        model = None
        model_names = [
            'gemini-flash-lite-latest',
            'gemini-2.0-flash-exp',  # ìµœì‹  Gemini 2.0 Flash ì‹¤í—˜ ë²„ì „
            'gemini-2.0-flash',       # Gemini 2.0 Flash
            'gemini-1.5-flash',       # Gemini 1.5 Flash (ë¹ ë¥´ê³  ì•ˆì •ì )
            'gemini-1.5-pro',         # Gemini 1.5 Pro
            'gemini-pro'              # êµ¬ë²„ì „ Gemini Pro
        ]
        
        last_error = None
        for model_name in model_names:
            try:
                model = genai.GenerativeModel(model_name)
                # ëª¨ë¸ ë¡œë”© ì„±ê³µ ì‹œ ë””ë²„ê·¸ ì •ë³´ (ì„ íƒì‚¬í•­)
                # st.info(f"âœ… Gemini ëª¨ë¸ ì‚¬ìš©: {model_name}")
                break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            except Exception as model_err:
                last_error = model_err
                continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
        
        if model is None:
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš°
            if last_error:
                st.warning(f"âš ï¸ Gemini ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(last_error)}")
            return summarize_transcript_fallback(transcript, max_length)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ëª©ì°¨ë³„ ì •ë¦¬)
        target_length = max_length
        prompt = f"""ë‹¤ìŒì€ ìœ íŠœë¸Œ ë™ì˜ìƒì˜ ìë§‰/ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì •í™•íˆ {target_length}ì ë‚´ì™¸(900-1100ì ë²”ìœ„)ë¡œ ëª©ì°¨ë³„ë¡œ ì •ë¦¬í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”.

ìŠ¤í¬ë¦½íŠ¸:
{transcript}

ìš”ì•½í•  ë•Œ ë‹¤ìŒ í˜•ì‹ê³¼ ê·œì¹™ì„ ì—„ê²©íˆ ì§€ì¼œì£¼ì„¸ìš”:

**í˜•ì‹:**
1. **[ì£¼ì œ1]**: í•µì‹¬ ë‚´ìš© ì„¤ëª… (2-3ë¬¸ì¥)
2. **[ì£¼ì œ2]**: í•µì‹¬ ë‚´ìš© ì„¤ëª… (2-3ë¬¸ì¥)
3. **[ì£¼ì œ3]**: í•µì‹¬ ë‚´ìš© ì„¤ëª… (2-3ë¬¸ì¥)
...

**ê·œì¹™:**
1. 4-6ê°œì˜ ì£¼ìš” ëª©ì°¨ë¡œ êµ¬ì„±
2. ê° ëª©ì°¨ëŠ” í•µì‹¬ ì£¼ì œë¥¼ í•œ ë‹¨ì–´ë‚˜ ì§§ì€ êµ¬ë¬¸ìœ¼ë¡œ í‘œí˜„
3. ê° ëª©ì°¨ë³„ ì„¤ëª…ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ
4. ì „ì²´ ê¸¸ì´ëŠ” ë°˜ë“œì‹œ {target_length}ì ë‚´ì™¸(900-1100ì ë²”ìœ„)ë¡œ ì‘ì„±
5. ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
6. ë¶ˆí•„ìš”í•œ ì„œë¬¸ì´ë‚˜ ì„¤ëª… ì—†ì´ ë°”ë¡œ ëª©ì°¨ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
7. êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì¤‘ìš”í•œ ìˆ«ìê°€ ìˆìœ¼ë©´ í¬í•¨

ìš”ì•½ (ë°˜ë“œì‹œ 900-1100ì ë²”ìœ„, ëª©ì°¨ë³„ í˜•ì‹):"""
        
        # ìš”ì•½ ìƒì„±
        response = model.generate_content(prompt)
        
        if response and response.text:
            summary = response.text.strip()
            # ìš”ì•½ ê¸¸ì´ ì¡°ì •
            summary_length = len(summary)
            
            # ë²”ìœ„ ë°–ì´ë©´ ì¡°ì • (900-1100ì ë²”ìœ„)
            if summary_length > 1100:
                # ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ì—ì„œ 1000ìê¹Œì§€ ìë¥´ê¸° (ë¬¸ì¥ ë‹¨ìœ„)
                sentences = re.split(r'([.!?]\s+)', summary)
                result = ""
                for i in range(0, len(sentences), 2):
                    if i + 1 < len(sentences):
                        sentence_pair = sentences[i] + sentences[i+1]
                    else:
                        sentence_pair = sentences[i]
                    
                    if len(result) + len(sentence_pair) <= 1000:
                        result += sentence_pair
                    else:
                        break
                summary = result.strip()
                if len(summary) < 900:
                    # ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ì—ì„œ ë” ì¶”ê°€
                    remaining = summary
                    for i in range(len(sentences) - len(result), len(sentences)):
                        if i + 1 < len(sentences):
                            sentence_pair = sentences[i] + sentences[i+1]
                        else:
                            sentence_pair = sentences[i]
                        if len(remaining) + len(sentence_pair) <= 1000:
                            remaining += sentence_pair
                        else:
                            break
                    summary = remaining.strip()
                if len(summary) > 1100:
                    summary = summary[:1000] + "..."
            
            elif summary_length < 800:
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë” ì¶”ê°€ (Geminiê°€ ì¶©ë¶„íˆ ìš”ì•½í•˜ì§€ ì•Šì€ ê²½ìš°)
                pass  # ê·¸ëŒ€ë¡œ ì‚¬ìš© (Gemini ìš”ì•½ ê²°ê³¼)
            
            return summary
        else:
            return summarize_transcript_fallback(transcript, max_length)
            
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ë°©ë²• ì‚¬ìš©
        st.warning(f"âš ï¸ Gemini API ìš”ì•½ ì‹¤íŒ¨, ê¸°ë³¸ ìš”ì•½ ë°©ë²• ì‚¬ìš©: {str(e)}")
        return summarize_transcript_fallback(transcript, max_length)

def summarize_transcript_fallback(transcript, max_length=1000):
    """
    ìë§‰/ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê¸°ë³¸ ë°©ë²•ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (Gemini ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    900-1100ì ë²”ìœ„ë¡œ ì—„ê²©í•˜ê²Œ ì œí•œ
    """
    if not transcript:
        return None
    
    # ëª©í‘œ ê¸¸ì´ ë²”ìœ„ ì„¤ì • (900-1100ì)
    target_min = 900
    target_max = 1100
    
    # ê¸¸ì´ê°€ ì´ë¯¸ ì ì ˆí•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if target_min <= len(transcript) <= target_max:
        return transcript
    
    # ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ìš”ì•½ ë¶ˆí•„ìš”)
    if len(transcript) < target_min:
        return transcript
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
    sentences = re.split(r'([.!?]\s+)', transcript)
    
    # ì²« ë¶€ë¶„ë¶€í„° ì°¨ë¡€ë¡œ ë”í•´ì„œ ëª©í‘œ ê¸¸ì´ ë²”ìœ„ê¹Œì§€
    summary = []
    current_length = 0
    
    # ë¬¸ì¥ê³¼ êµ¬ë¶„ìë¥¼ ì§ìœ¼ë¡œ ì²˜ë¦¬
    for i in range(0, len(sentences), 2):
        if i < len(sentences):
            sentence = sentences[i].strip()
            if i + 1 < len(sentences):
                separator = sentences[i + 1]
            else:
                separator = ""
            
            if not sentence:
                continue
            
            sentence_with_sep = sentence + separator
            sentence_length = len(sentence_with_sep)
            
            # ëª©í‘œ ë²”ìœ„ë¥¼ ë„˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
            if current_length + sentence_length <= target_max:
                summary.append(sentence_with_sep)
                current_length += sentence_length
            else:
                # ëª©í‘œ ë²”ìœ„ì— ê°€ê¹Œì›Œì¡ŒëŠ”ì§€ í™•ì¸
                if current_length >= target_min:
                    break
                # ì•„ì§ ëª©í‘œ ë²”ìœ„ ë¯¸ë§Œì´ë©´ ì¶”ê°€ (í•˜ì§€ë§Œ ìµœëŒ€ê°’ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡)
                if current_length + sentence_length <= target_max:
                    summary.append(sentence_with_sep)
                    current_length += sentence_length
                    break
    
    result = ''.join(summary).strip()
    
    # ìµœì¢… ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
    if len(result) > target_max:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        sentences_result = re.split(r'([.!?]\s+)', result)
        trimmed = ""
        for i in range(0, len(sentences_result), 2):
            if i < len(sentences_result):
                sentence = sentences_result[i]
                if i + 1 < len(sentences_result):
                    separator = sentences_result[i + 1]
                else:
                    separator = ""
                sentence_with_sep = sentence + separator
                
                if len(trimmed) + len(sentence_with_sep) <= target_max:
                    trimmed += sentence_with_sep
                else:
                    break
        result = trimmed.strip()
        if len(result) > target_max:
            result = result[:1000] + "..."
    
    # ìµœì†Œ ê¸¸ì´ í™•ì¸
    if len(result) < target_min and len(transcript) > target_min:
        # ì›ë³¸ì—ì„œ ë” ê°€ì ¸ì˜¤ê¸° (ë‹¨, ìµœëŒ€ê°’ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡)
        remaining_sentences = re.split(r'([.!?]\s+)', transcript[len(''.join(summary)):])
        for i in range(0, len(remaining_sentences), 2):
            if i < len(remaining_sentences):
                sentence = remaining_sentences[i].strip()
                if i + 1 < len(remaining_sentences):
                    separator = remaining_sentences[i + 1]
                else:
                    separator = ""
                
                if not sentence:
                    continue
                
                sentence_with_sep = sentence + separator
                if len(result) + len(sentence_with_sep) <= target_max:
                    result += sentence_with_sep
                else:
                    break
    
    return result

def summarize_transcript(transcript, max_length=1000):
    """
    ìë§‰/ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (Gemini ìš°ì„  ì‚¬ìš©, 1000ì ë‚´ì™¸, ëª©ì°¨ë³„ ì •ë¦¬)
    """
    return summarize_transcript_with_gemini(transcript, max_length)

def get_youtube_summary(youtube_url, person_name):
    """
    ìœ íŠœë¸Œ ì±„ë„/ë™ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    """
    if not youtube_url or 'youtube.com/results' in youtube_url:
        # ê²€ìƒ‰ URLì¸ ê²½ìš° ìš”ì•½ ì •ë³´ ì—†ìŒ
        return None
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        response = requests.get(youtube_url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        summary = {
            'channel_title': None,
            'description': None,
            'subscriber_count': None,
            'video_count': None,
            'recent_videos': []
        }
        
        # ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ì—ì„œ JSON ë°ì´í„° ì°¾ê¸°
        scripts = soup.find_all('script')
        page_data = None
        
        for script in scripts:
            if script.string and ('var ytInitialData' in script.string or 'window["ytInitialData"]' in script.string):
                # ìœ íŠœë¸Œ ì´ˆê¸° ë°ì´í„° ì¶”ì¶œ ì‹œë„
                try:
                    # JSON ë°ì´í„° ì¶”ì¶œ
                    script_text = script.string
                    if 'var ytInitialData' in script_text:
                        start_idx = script_text.find('var ytInitialData = ') + len('var ytInitialData = ')
                        end_idx = script_text.find(';</script>', start_idx)
                        if end_idx == -1:
                            end_idx = script_text.find('};', start_idx) + 1
                        if end_idx > start_idx:
                            json_str = script_text[start_idx:end_idx]
                            try:
                                page_data = json.loads(json_str)
                            except:
                                pass
                except:
                    pass
        
        # ì±„ë„ ì œëª© ì°¾ê¸°
        title = soup.find('meta', property='og:title')
        if title:
            summary['channel_title'] = title.get('content', '')
        
        # ì„¤ëª… ì°¾ê¸°
        desc = soup.find('meta', property='og:description')
        if desc:
            summary['description'] = desc.get('content', '')
        
        # ì±„ë„ ì •ë³´ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ë…ì ìˆ˜, ë™ì˜ìƒ ìˆ˜ ì°¾ê¸°
        if page_data:
            try:
                # ì±„ë„ ì •ë³´ ì¶”ì¶œ (JSON êµ¬ì¡°ê°€ ë³µì¡í•˜ë¯€ë¡œ ì¼ë°˜ì ì¸ ê²½ë¡œ ì‹œë„)
                channel_info_text = str(page_data)
                # êµ¬ë…ì ìˆ˜ íŒ¨í„´
                subscriber_pattern = r'(\d+(?:\.\d+)?[ë§Œì²œì–µê°œ]*)\s*ëª…?\s*êµ¬ë…'
                subscriber_match = re.search(subscriber_pattern, channel_info_text, re.IGNORECASE)
                if subscriber_match:
                    summary['subscriber_count'] = subscriber_match.group(1)
                
                # ë™ì˜ìƒ ìˆ˜ íŒ¨í„´
                video_pattern = r'ë™ì˜ìƒ\s*(\d+(?:,\d+)*)'
                video_match = re.search(video_pattern, channel_info_text)
                if video_match:
                    summary['video_count'] = video_match.group(1)
            except:
                pass
        
        # í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì°¾ê¸°
        page_text = soup.get_text()
        
        # êµ¬ë…ì ìˆ˜ ì°¾ê¸°
        if not summary['subscriber_count']:
            subscriber_patterns = [
                r'êµ¬ë…ì\s*(\d+(?:\.\d+)?[ë§Œì²œì–µ]*)\s*ëª…',
                r'(\d+(?:\.\d+)?[ë§Œì²œì–µ]*)\s*êµ¬ë…ì',
                r'Subscribers:\s*(\d+(?:,\d+)*)'
            ]
            for pattern in subscriber_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    summary['subscriber_count'] = match.group(1)
                    break
        
        # ë™ì˜ìƒ ìˆ˜ ì°¾ê¸°
        if not summary['video_count']:
            video_patterns = [
                r'ë™ì˜ìƒ\s*(\d+(?:,\d+)*)\s*ê°œ',
                r'(\d+(?:,\d+)*)\s*ë™ì˜ìƒ',
                r'Videos:\s*(\d+(?:,\d+)*)'
            ]
            for pattern in video_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    summary['video_count'] = match.group(1)
                    break
        
        # ìµœê·¼ ë™ì˜ìƒ ì œëª© ì°¾ê¸° (ê°„ë‹¨í•˜ê²Œ)
        video_titles = []
        # ë©”íƒ€ íƒœê·¸ë‚˜ ì œëª©ì—ì„œ ë™ì˜ìƒ ì •ë³´ ì°¾ê¸°
        meta_tags = soup.find_all('meta', {'property': 'og:title'})
        for meta in meta_tags:
            title_text = meta.get('content', '')
            if title_text and title_text != summary['channel_title']:
                video_titles.append(title_text)
                if len(video_titles) >= 3:
                    break
        
        summary['recent_videos'] = video_titles[:3]
        
        # ë¹„ë””ì˜¤ URLì¸ ê²½ìš° ìë§‰/ìŠ¤í¬ë¦½íŠ¸ ê°€ì ¸ì˜¤ê¸°
        video_id = extract_video_id_from_url(youtube_url)
        
        # ë¹„ë””ì˜¤ IDê°€ ì—†ìœ¼ë©´ ì±„ë„ URLë¡œ ê°„ì£¼í•˜ê³  ìµœì‹  ë™ì˜ìƒ ì°¾ê¸°
        if not video_id:
            # ì±„ë„ URLì¸ì§€ í™•ì¸
            is_channel = any(x in youtube_url for x in ['/channel/', '/c/', '/@', '/user/'])
            if is_channel:
                # ì±„ë„ì—ì„œ ìµœì‹  ë™ì˜ìƒ ì°¾ê¸°
                video_id = get_latest_video_from_channel(youtube_url)
        
        # ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ìš”ì•½ ì €ì¥
        summary['transcript_raw'] = None
        summary['transcript_summary'] = None
        summary['video_id_used'] = video_id  # ë””ë²„ê¹…ìš©
        
        if video_id:
            # ìŠ¤í¬ë¦½íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            try:
                transcript = get_youtube_transcript(video_id)
                if transcript and len(transcript.strip()) > 50:  # ì˜ë¯¸ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ì¸ì§€ í™•ì¸
                    # ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ (ìš”ì•½ ì—†ì´ ê·¸ëŒ€ë¡œ)
                    summary['transcript_raw'] = transcript
                    # ìš”ì•½ë„ ìƒì„± (1000ì ë‚´ì™¸, ëª©ì°¨ë³„ ì •ë¦¬)
                    try:
                        summary['transcript_summary'] = summarize_transcript(transcript, max_length=1000)
                    except Exception as sum_err:
                        summary['transcript_summary'] = None
                        summary['error_summary'] = f"ìš”ì•½ ì‹¤íŒ¨: {str(sum_err)}"
                else:
                    summary['error_transcript'] = "ìŠ¤í¬ë¦½íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤"
            except Exception as trans_err:
                summary['error_transcript'] = f"ìŠ¤í¬ë¦½íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(trans_err)}"
        
        return summary
        
    except Exception as e:
        return None

def display_youtube_list_and_summary(youtube_links, person_name, instructor_name):
    """
    ìœ íŠœë¸Œ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•˜ê³  ì„ íƒëœ ë§í¬ì˜ ìš”ì•½ ì •ë³´ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    if not youtube_links:
        st.markdown("---")
        st.info("ğŸ’¡ ê´€ë ¨ì„±ì´ ë†’ì€ ìœ íŠœë¸Œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.markdown("---")
    
    # ë””ë²„ê¹…: ì°¾ì€ ë§í¬ ê°œìˆ˜ í‘œì‹œ
    video_count = len([l for l in youtube_links if l['type'] == 'video'])
    channel_count = len([l for l in youtube_links if l['type'] == 'channel'])
    st.markdown(f"**ğŸ“º ìœ íŠœë¸Œ ê²€ìƒ‰ ê²°ê³¼:** ë™ì˜ìƒ {video_count}ê°œ, ì±„ë„ {channel_count}ê°œ")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì„ íƒëœ ìœ íŠœë¸Œ URL ì €ì¥ìš©)
    selected_youtube_key = f"selected_youtube_{instructor_name}"
    if selected_youtube_key not in st.session_state:
        st.session_state[selected_youtube_key] = None
    
    # ê²€ìƒ‰ URLë§Œ ìˆëŠ” ê²½ìš° (ì‹¤ì œ ë§í¬ë¥¼ ëª» ì°¾ìŒ)
    if len(youtube_links) == 1 and youtube_links[0]['type'] == 'search':
        youtube_link = youtube_links[0]
        st.markdown(f"[{youtube_link['title']}]({youtube_link['url']})")
        st.info("ğŸ’¡ ìœ íŠœë¸Œì—ì„œ ì§ì ‘ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")
        return
    
    # ìœ íŠœë¸Œ ë§í¬ê°€ 1ê°œë§Œ ìˆëŠ” ê²½ìš°
    if len(youtube_links) == 1:
        youtube_link = youtube_links[0]
        st.markdown(f"**ì œëª©:** {youtube_link['title']}")
        st.markdown(f"[ğŸ”— ë§í¬ ì—´ê¸°]({youtube_link['url']})")
        display_youtube_summary(youtube_link['url'], person_name)
        return
    
    # ì—¬ëŸ¬ ê°œì˜ ìœ íŠœë¸Œ ë§í¬ê°€ ìˆëŠ” ê²½ìš°
    st.markdown(f"**ğŸ“‹ ì´ {len(youtube_links)}ê°œì˜ ë§í¬ (ìµœì‹ ìˆœ):**")
    st.caption("ğŸ’¡ ì›í•˜ëŠ” ë™ì˜ìƒì„ í´ë¦­í•˜ë©´ í•´ë‹¹ ì œëª© ë°”ë¡œ ì•„ë˜ì— ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½ì´ í‘œì‹œë©ë‹ˆë‹¤")
    
    # ë™ì˜ìƒê³¼ ì±„ë„ ë¶„ë¦¬
    video_links = [link for link in youtube_links if link['type'] == 'video']
    channel_links = [link for link in youtube_links if link['type'] == 'channel']
    
    # ë™ì˜ìƒ ë¦¬ìŠ¤íŠ¸ (ê° ë™ì˜ìƒ ì•„ë˜ì— ì„ íƒ ì‹œ ìš”ì•½ í‘œì‹œ)
    if video_links:
        st.markdown(f"### ğŸ¬ ë™ì˜ìƒ ({len(video_links)}ê°œ)")
        for idx, link in enumerate(video_links[:10]):  # ìµœëŒ€ 10ê°œ
            # ë²„íŠ¼ í‘œì‹œ (ë‚ ì§œ í¬í•¨)
            display_title = link['title']
            if len(display_title) > 60:
                display_title = display_title[:57] + "..."
            
            # ë‚ ì§œ ì •ë³´ ì¶”ê°€
            if link.get('published'):
                button_text = f"â–¶ï¸ {display_title}  ğŸ“… {link['published']}"
            else:
                button_text = f"â–¶ï¸ {display_title}"
            
            button_clicked = st.button(
                button_text, 
                key=f"video_{instructor_name}_{idx}", 
                use_container_width=True
            )
            
            if button_clicked:
                if st.session_state[selected_youtube_key] == link['url']:
                    # ì´ë¯¸ ì„ íƒëœ í•­ëª©ì„ ë‹¤ì‹œ í´ë¦­í•˜ë©´ ì„ íƒ ì·¨ì†Œ
                    st.session_state[selected_youtube_key] = None
                else:
                    # ìƒˆë¡œìš´ í•­ëª© ì„ íƒ
                    st.session_state[selected_youtube_key] = link['url']
                st.rerun()
            
            # ì„ íƒëœ ë™ì˜ìƒì´ë©´ ë°”ë¡œ ì•„ë˜ì— ìš”ì•½ í‘œì‹œ
            if st.session_state[selected_youtube_key] == link['url']:
                st.markdown('<div style="background-color: #f0f7ff; padding: 1rem; border-radius: 8px; border-left: 4px solid #667eea; margin: 0.5rem 0 1rem 2rem;">', unsafe_allow_html=True)
                
                st.markdown(f"**âœ… ì„ íƒëœ ë™ì˜ìƒ:** {link['title']}")
                if link.get('published'):
                    st.markdown(f"**ğŸ“… ê²Œì‹œì¼:** {link['published']}")
                st.markdown(f"[ğŸ”— ìœ íŠœë¸Œì—ì„œ ë³´ê¸°]({link['url']})")
                
                # ì„ íƒ ì·¨ì†Œ ë²„íŠ¼
                if st.button("âŒ ì„ íƒ ì·¨ì†Œ", key=f"clear_video_{instructor_name}_{idx}"):
                    st.session_state[selected_youtube_key] = None
                    st.rerun()
                
                st.markdown('</div>', unsafe_allow_html=True)
                
                # ìš”ì•½ ì •ë³´ í‘œì‹œ (ë™ì˜ìƒ ë°”ë¡œ ì•„ë˜)
                st.markdown('<div style="margin-left: 2rem;">', unsafe_allow_html=True)
                display_youtube_summary(link['url'], person_name)
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("---")
    
    # ì±„ë„ ë¦¬ìŠ¤íŠ¸ (ë™ì˜ìƒê³¼ ë¶„ë¦¬)
    if channel_links:
        st.markdown(f"### ğŸ“¢ ì±„ë„ ({len(channel_links)}ê°œ)")
        for idx, link in enumerate(channel_links[:10]):  # ìµœëŒ€ 10ê°œ
            # ë²„íŠ¼ í‘œì‹œ
            display_title = link['title']
            if len(display_title) > 60:
                display_title = display_title[:57] + "..."
            
            button_clicked = st.button(
                f"ğŸ“º {display_title}", 
                key=f"channel_{instructor_name}_{idx}", 
                use_container_width=True
            )
            
            if button_clicked:
                if st.session_state[selected_youtube_key] == link['url']:
                    # ì´ë¯¸ ì„ íƒëœ í•­ëª©ì„ ë‹¤ì‹œ í´ë¦­í•˜ë©´ ì„ íƒ ì·¨ì†Œ
                    st.session_state[selected_youtube_key] = None
                else:
                    # ìƒˆë¡œìš´ í•­ëª© ì„ íƒ
                    st.session_state[selected_youtube_key] = link['url']
                st.rerun()
            
            # ì„ íƒëœ ì±„ë„ì´ë©´ ë°”ë¡œ ì•„ë˜ì— ìš”ì•½ í‘œì‹œ
            if st.session_state[selected_youtube_key] == link['url']:
                st.markdown('<div style="background-color: #f0f7ff; padding: 1rem; border-radius: 8px; border-left: 4px solid #667eea; margin: 0.5rem 0 1rem 2rem;">', unsafe_allow_html=True)
                
                st.markdown(f"**âœ… ì„ íƒëœ ì±„ë„:** {link['title']}")
                st.markdown(f"[ğŸ”— ìœ íŠœë¸Œì—ì„œ ë³´ê¸°]({link['url']})")
                
                # ì„ íƒ ì·¨ì†Œ ë²„íŠ¼
                if st.button("âŒ ì„ íƒ ì·¨ì†Œ", key=f"clear_channel_{instructor_name}_{idx}"):
                    st.session_state[selected_youtube_key] = None
                    st.rerun()
                
                st.markdown('</div>', unsafe_allow_html=True)
                
                # ìš”ì•½ ì •ë³´ í‘œì‹œ (ì±„ë„ ë°”ë¡œ ì•„ë˜)
                st.markdown('<div style="margin-left: 2rem;">', unsafe_allow_html=True)
                display_youtube_summary(link['url'], person_name)
                st.markdown('</div>', unsafe_allow_html=True)
                st.markdown("---")

def display_youtube_summary(youtube_url, person_name):
    """
    ì„ íƒëœ ìœ íŠœë¸Œì˜ ìš”ì•½ ì •ë³´ë¥¼ UIì— í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    if not youtube_url:
        return
    
    # ìš”ì•½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    summary_cache_key = f"youtube_summary_{youtube_url}"
    if summary_cache_key not in st.session_state:
        with st.spinner("ìœ íŠœë¸Œ ì±„ë„ ì •ë³´ ë° ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            summary = get_youtube_summary(youtube_url, person_name)
            st.session_state[summary_cache_key] = summary
    else:
        summary = st.session_state[summary_cache_key]
    
    if not summary:
        st.warning("âš ï¸ ìœ íŠœë¸Œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìš”ì•½ ì •ë³´ë¥¼ ë°•ìŠ¤ë¡œ í‘œì‹œ
    st.markdown('<div style="background-color: #ffffff; padding: 1.5rem; border-radius: 10px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin: 1rem 0;">', unsafe_allow_html=True)
    
    summary_text = []
    
    if summary.get('channel_title'):
        summary_text.append(f"â€¢ **ì±„ë„ëª…:** {summary['channel_title']}")
    
    if summary.get('subscriber_count'):
        summary_text.append(f"â€¢ **êµ¬ë…ì ìˆ˜:** {summary['subscriber_count']}")
    
    if summary.get('video_count'):
        summary_text.append(f"â€¢ **ë™ì˜ìƒ ìˆ˜:** {summary['video_count']}ê°œ")
    
    if summary_text:
        st.markdown("**ğŸ“º ì±„ë„ ì •ë³´:**")
        st.markdown("\n".join(summary_text))
    
    # ì£¼ìš” ë‚´ìš© í‘œì‹œ (ìš°ì„ ìˆœìœ„: ìš”ì•½ > ì„¤ëª…)
    if summary.get('transcript_summary'):
        # ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½ì´ ìˆìœ¼ë©´ í‘œì‹œ
        st.markdown("---")
        st.markdown("### ğŸ“‹ ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½ (1000ì ëª©ì°¨ë³„)")
        st.markdown(summary['transcript_summary'])
        
        # ë¹„ë””ì˜¤ ID í‘œì‹œ (ë””ë²„ê¹…ìš©)
        if summary.get('video_id_used'):
            st.caption(f"âœ… ë¹„ë””ì˜¤ ID: {summary['video_id_used']}")
        
        # ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë³´ê¸° (ì ‘ì„ ìˆ˜ ìˆê²Œ)
        if summary.get('transcript_raw'):
            with st.expander("ğŸ“ ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ì „ì²´ ë³´ê¸°"):
                st.text_area("", value=summary['transcript_raw'], height=400, disabled=True, label_visibility="collapsed")
    
    elif summary.get('transcript_raw'):
        # ìš”ì•½ì€ ì—†ì§€ë§Œ ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        st.markdown("---")
        st.markdown(f"**ğŸ“‹ ì£¼ìš” ë‚´ìš© (ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸):**")
        st.text_area("", value=summary['transcript_raw'], height=300, disabled=True, label_visibility="collapsed")
        if summary.get('video_id_used'):
            st.caption(f"âœ… ë¹„ë””ì˜¤ ID: {summary['video_id_used']}")
    
    elif summary.get('description'):
        # ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìœ¼ë©´ ì„¤ëª…ì„ ì£¼ìš” ë‚´ìš©ìœ¼ë¡œ í‘œì‹œ
        st.markdown("---")
        desc = summary['description']
        if len(desc) > 1000:
            desc = desc[:1000] + "..."
        st.markdown(f"**ğŸ“‹ ì£¼ìš” ë‚´ìš©:**\n\n{desc}")
        
        # ì—ëŸ¬ ì •ë³´ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
        if summary.get('error_transcript'):
            st.info(f"â„¹ï¸ {summary['error_transcript']}")
        if summary.get('video_id_used'):
            st.caption(f"ğŸ” ì‹œë„í•œ ë¹„ë””ì˜¤ ID: {summary['video_id_used']}")
    
    else:
        # ì•„ë¬´ê²ƒë„ ì—†ëŠ” ê²½ìš°
        if summary.get('error_transcript'):
            st.warning(f"âš ï¸ {summary['error_transcript']}")
        if summary.get('video_id_used'):
            st.info(f"ğŸ” ì‹œë„í•œ ë¹„ë””ì˜¤ ID: {summary['video_id_used']}")
    
    # ìµœê·¼ ë™ì˜ìƒ ì •ë³´
    if summary.get('recent_videos'):
        st.markdown("---")
        st.markdown("**ğŸ“¹ ìµœê·¼ ë™ì˜ìƒ:**")
        for video in summary['recent_videos'][:3]:
            st.markdown(f"  - {video}")
    
    # ë°•ìŠ¤ ë‹«ê¸°
    st.markdown('</div>', unsafe_allow_html=True)

# Session state ì´ˆê¸°í™”
if 'selected_instructor' not in st.session_state:
    st.session_state.selected_instructor = None
if 'selected_instructor_idx' not in st.session_state:
    st.session_state.selected_instructor_idx = None
if 'search_results' not in st.session_state:
    st.session_state.search_results = pd.DataFrame()
if 'web_search_result' not in st.session_state:
    st.session_state.web_search_result = None
if 'last_search_query' not in st.session_state:
    st.session_state.last_search_query = None
if 'last_search_type' not in st.session_state:
    st.session_state.last_search_type = None

# ë©”ì¸ UI
st.markdown('<div class="main-title"><h1>ğŸ” ê°•ì‚¬ ê³ ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œ</h1><p>ê°•ì‚¬ì´ë¦„, ë¶„ì•¼, ë˜ëŠ” ê°•ì˜ ê³¼ëª©ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ì„ íƒí•˜ì„¸ìš”</p></div>', unsafe_allow_html=True)

# CSV ì—…ë¡œë“œ ëŒ€ì•ˆ ì œê³µ
uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)", type=['csv'], help="Google Sheetsì—ì„œ ë‹¤ìš´ë¡œë“œí•œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ë°ì´í„° ë¡œë“œ
with st.spinner("ê°•ì‚¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            df.columns = df.columns.str.strip()
            st.success(f"CSV íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤. ({len(df)}ê°œ í–‰)")
        except Exception as e:
            st.error(f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            df = pd.DataFrame()
    else:
        df = load_instructor_data()

if df.empty:
    st.error("ê°•ì‚¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    with st.expander("ğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²• ë³´ê¸°"):
        st.markdown("""
        ### Google Sheets ì ‘ê·¼ ê¶Œí•œ ì„¤ì • í•„ìš”
        
        **âš ï¸ ì¤‘ìš”: Excel íŒŒì¼(.xlsx)ì¸ ê²½ìš°**
        - Google Sheets í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”
        1. Google Driveì—ì„œ íŒŒì¼ ì—´ê¸°
        2. "Google ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¡œ ì—´ê¸°" í´ë¦­
        3. íŒŒì¼ > Google ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¡œ ì €ì¥
        
        **ë°©ë²• 1: ì„œë¹„ìŠ¤ ê³„ì • ê¶Œí•œ ë¶€ì—¬ (ê¶Œì¥)**
        1. Google Sheets íŒŒì¼ ì—´ê¸°
        2. "ê³µìœ " ë²„íŠ¼ í´ë¦­
        3. ì´ë©”ì¼ ì¶”ê°€: `ai-coding@huhsame-project-1.iam.gserviceaccount.com`
        4. ê¶Œí•œ: "ë·°ì–´" ì„ íƒ
        5. "ì „ì†¡" í´ë¦­
        
        **ë°©ë²• 2: ê³µê°œ ì‹œíŠ¸ë¡œ ì „í™˜**
        1. Google Sheets íŒŒì¼ ì—´ê¸°
        2. "ê³µìœ " ë²„íŠ¼ í´ë¦­
        3. "ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì" ì„¤ì •
        4. "ëœë¤í•œ ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì"ë¥¼ "ë·°ì–´"ë¡œ ë³€ê²½
        
        **ë°©ë²• 3: CSVë¡œ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ**
        1. Google Sheetsì—ì„œ "íŒŒì¼ > ë‹¤ìš´ë¡œë“œ > ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê°’(.csv)" ì„ íƒ
        2. CSV íŒŒì¼ì„ ì•±ì— ì—…ë¡œë“œí•˜ì—¬ ì‚¬ìš©
        """)
    
    st.stop()

# ê´€ë¦¬ììš© ê°•ì‚¬ ì •ë³´ ì—…ë¡œë“œ ì„¹ì…˜
st.markdown("---")
st.markdown("### ğŸ› ï¸ ê´€ë¦¬ì ê¸°ëŠ¥")

with st.expander("ğŸ“¤ ê°•ì‚¬ ì •ë³´ ì—…ë¡œë“œ", expanded=False):
    st.markdown("**Google Sheetsì— ê°•ì‚¬ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.**")
    
    # íƒ­ ìƒì„±
    tab1, tab2 = st.tabs(["ìƒˆ ê°•ì‚¬ ì¶”ê°€", "ì—‘ì…€ ì¼ê´„ ì—…ë¡œë“œ"])
    
    with tab1:
        st.markdown("#### ê°œë³„ ê°•ì‚¬ ì •ë³´ ì…ë ¥")
        
        with st.form("instructor_form", clear_on_submit=False):
            col1, col2 = st.columns(2)
            
            with col1:
                instr_name = st.text_input("ê°•ì‚¬ ì´ë¦„ *", key="form_name")
                affiliation = st.text_input("ì†Œì† *", key="form_affiliation")
                job = st.text_input("ì§ì—…", key="form_job")
                subject = st.text_input("ê°•ì˜ ê³¼ëª©", key="form_subject")
                email = st.text_input("ì´ë©”ì¼ ì£¼ì†Œ", key="form_email")
                main_category = st.text_input("ëŒ€ë¶„ì•¼", key="form_main_cat")
            
            with col2:
                sub_category = st.text_input("ì†Œë¶„ì•¼", key="form_sub_cat")
                satisfaction = st.text_input("ë§Œì¡±ë„", key="form_satisfaction")
                feedback = st.text_area("í•™ìŠµì ì£¼ìš” ì˜ê²¬", height=100, key="form_feedback")
                manager_comment = st.text_area("ë‹´ë‹¹ì ì˜ê²¬", height=100, key="form_manager")
            
            st.caption("ğŸ’¡ í•„ìˆ˜ í•­ëª©(*)ë§Œ ì…ë ¥í•´ë„ ì €ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            
            submitted = st.form_submit_button("ğŸ’¾ ê°•ì‚¬ ì •ë³´ ì €ì¥", use_container_width=True, type="primary")
            
            if submitted:
                if not instr_name or not affiliation:
                    st.error("ê°•ì‚¬ ì´ë¦„ê³¼ ì†Œì†ì€ í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤.")
                else:
                    # TODO: ì‹¤ì œë¡œ Google Sheetsì— ì €ì¥í•˜ëŠ” ë¡œì§ êµ¬í˜„
                    st.success(f"âœ… '{instr_name}' ê°•ì‚¬ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.info("âš ï¸ í˜„ì¬ëŠ” ë°ëª¨ ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ ì €ì¥ ê¸°ëŠ¥ì€ Google Sheets API ì„¤ì • í›„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    with tab2:
        st.markdown("#### ì—‘ì…€/CSV íŒŒì¼ ì—…ë¡œë“œ")
        
        uploaded_batch = st.file_uploader(
            "ì—‘ì…€(.xlsx) ë˜ëŠ” CSV(.csv) íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['xlsx', 'xls', 'csv'],
            help="ì—¬ëŸ¬ ê°•ì‚¬ ì •ë³´ê°€ í¬í•¨ëœ íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        if uploaded_batch is not None:
            try:
                # íŒŒì¼ í™•ì¥ì í™•ì¸
                file_extension = uploaded_batch.name.split('.')[-1].lower()
                
                if file_extension == 'csv':
                    batch_df = pd.read_csv(uploaded_batch)
                elif file_extension in ['xlsx', 'xls']:
                    batch_df = pd.read_excel(uploaded_batch)
                else:
                    st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
                    batch_df = None
                
                if batch_df is not None and not batch_df.empty:
                    st.success(f"âœ… íŒŒì¼ ì½ê¸° ì„±ê³µ! {len(batch_df)}ê°œ í–‰ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                    
                    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                    with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                        st.dataframe(batch_df.head(10), use_container_width=True)
                    
                    # ì—…ë¡œë“œ ë²„íŠ¼
                    if st.button("â¬†ï¸ ì—‘ì…€ ë°ì´í„° ì—…ë¡œë“œ", type="primary", use_container_width=True):
                        # TODO: ì‹¤ì œë¡œ Google Sheetsì— ì €ì¥í•˜ëŠ” ë¡œì§ êµ¬í˜„
                        st.success(f"âœ… {len(batch_df)}ê°œ ê°•ì‚¬ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.info("âš ï¸ í˜„ì¬ëŠ” ë°ëª¨ ëª¨ë“œì…ë‹ˆë‹¤. ì‹¤ì œ ì €ì¥ ê¸°ëŠ¥ì€ Google Sheets API ì„¤ì • í›„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            
            except Exception as e:
                st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")

st.markdown("---")

# ê²€ìƒ‰ ì…ë ¥ ì„¹ì…˜
col1, col2 = st.columns([4, 1])

with col1:
    search_query = st.text_input(
        "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: ê¹€ì–‘ë¯¼, ë§ˆì¼€íŒ…, ì „ëµ, Management ë“±",
        key="search_input"
    )

with col2:
    search_type = st.selectbox(
        "ê²€ìƒ‰ ë²”ìœ„",
        options=['all', 'name', 'field', 'subject'],
        format_func=lambda x: {
            'all': 'ì „ì²´',
            'name': 'ê°•ì‚¬ì´ë¦„',
            'field': 'ë¶„ì•¼',
            'subject': 'ê°•ì˜ê³¼ëª©'
        }[x],
        key="search_type"
    )

# ê²€ìƒ‰ ë²„íŠ¼
search_button = st.button("ğŸ” ê²€ìƒ‰", type="primary", use_container_width=True)

# ê²€ìƒ‰ ì‹¤í–‰ ë° ê²°ê³¼ í‘œì‹œ
if search_button and search_query:
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        results = search_instructors(df, search_query, search_type)
        st.session_state.search_results = results
        st.session_state.web_search_result = None  # ì´ˆê¸°í™”
        
        # ê²€ìƒ‰ì–´ì™€ íƒ€ì…ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (rerun í›„ì—ë„ ìœ ì§€)
        st.session_state.last_search_query = search_query
        st.session_state.last_search_type = search_type
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê³ , ê²€ìƒ‰ íƒ€ì…ì´ ì´ë¦„ ê²€ìƒ‰ì¸ ê²½ìš° ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰ ì‹œë„
        if results.empty and (search_type == 'name' or search_type == 'all'):
            with st.spinner("ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                web_result = search_naver_person(search_query)
                if web_result:
                    st.session_state.web_search_result = web_result
    # ìƒˆ ê²€ìƒ‰ ì‹œ ìƒì„¸ ì •ë³´ ì´ˆê¸°í™”
    st.session_state.selected_instructor = None
    st.session_state.selected_instructor_idx = None

# ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
if not st.session_state.search_results.empty:
    results = st.session_state.search_results
    
    if not results.empty:
        st.markdown(f"### ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ëª…)")
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        for idx, instructor in results.iterrows():
            with st.container():
                # ì»¬ëŸ¼ëª… ì°¾ê¸°
                name_col = [col for col in instructor.index if 'ê°•ì‚¬' in col and 'ì´ë¦„' in col]
                affiliation_col = [col for col in instructor.index if 'ì†Œì†' in col]
                job_col = [col for col in instructor.index if 'ì§ì—…' in col]
                
                name = instructor[name_col[0]] if name_col and pd.notna(instructor[name_col[0]]) else "ì´ë¦„ ì—†ìŒ"
                affiliation = instructor[affiliation_col[0]] if affiliation_col and pd.notna(instructor[affiliation_col[0]]) else "ì†Œì† ì •ë³´ ì—†ìŒ"
                job = instructor[job_col[0]] if job_col and pd.notna(instructor[job_col[0]]) else "ì§ì—… ì •ë³´ ì—†ìŒ"
                
                # ì¹´ë“œ í‘œì‹œ
                st.markdown('<div class="instructor-card">', unsafe_allow_html=True)
                
                col_name, col_detail = st.columns([1, 2])
                
                with col_name:
                    st.markdown(f"#### ğŸ‘¤ **{name}**")
                
                with col_detail:
                    st.markdown(f"**ğŸ¢ ì†Œì†:** {affiliation}  |  **ğŸ’¼ ì§ì—…:** {job}")
                
                # ì„ íƒ ë²„íŠ¼ (í† ê¸€ ê¸°ëŠ¥)
                button_text = "âŒ ìƒì„¸ ì •ë³´ ë‹«ê¸°" if st.session_state.selected_instructor_idx == idx else "ğŸ“– ìƒì„¸ ì •ë³´ ë³´ê¸°"
                if st.button(button_text, key=f"detail_{idx}", use_container_width=True):
                    if st.session_state.selected_instructor_idx == idx:
                        # ì´ë¯¸ ì„ íƒëœ í•­ëª©ì´ë©´ ë‹«ê¸°
                        st.session_state.selected_instructor = None
                        st.session_state.selected_instructor_idx = None
                    else:
                        # ìƒˆë¡œìš´ í•­ëª© ì„ íƒ
                        st.session_state.selected_instructor = instructor.to_dict()
                        st.session_state.selected_instructor_idx = idx
                    st.rerun()
                
                st.markdown('</div>', unsafe_allow_html=True)
                
                # ì„ íƒëœ í•­ëª©ì´ë©´ ë°”ë¡œ ì•„ë˜ì— ìƒì„¸ ì •ë³´ í‘œì‹œ
                if st.session_state.selected_instructor_idx == idx and st.session_state.selected_instructor is not None:
                    st.markdown("---")
                    st.markdown("### ğŸ“– ê°•ì‚¬ ìƒì„¸ ì •ë³´")
                    
                    st.markdown('<div class="info-card">', unsafe_allow_html=True)
                    
                    # ê°•ì‚¬ ì´ë¦„
                    name_cols = [key for key in st.session_state.selected_instructor.keys() if 'ê°•ì‚¬' in key and 'ì´ë¦„' in key]
                    if name_cols:
                        st.markdown(f"#### ğŸ‘¤ **{st.session_state.selected_instructor[name_cols[0]]}**")
                    
                    # ì†Œì†
                    affiliation_cols = [key for key in st.session_state.selected_instructor.keys() if 'ì†Œì†' in key]
                    if affiliation_cols and pd.notna(st.session_state.selected_instructor[affiliation_cols[0]]):
                        st.markdown(f"**ğŸ¢ ì†Œì†:** {st.session_state.selected_instructor[affiliation_cols[0]]}")
                    
                    # ì§ì—…
                    job_cols = [key for key in st.session_state.selected_instructor.keys() if 'ì§ì—…' in key]
                    if job_cols and pd.notna(st.session_state.selected_instructor[job_cols[0]]):
                        st.markdown(f"**ğŸ’¼ ì§ì—…:** {st.session_state.selected_instructor[job_cols[0]]}")
                    
                    # ê°•ì˜ ê³¼ëª©
                    subject_cols = [key for key in st.session_state.selected_instructor.keys() if 'ê°•ì˜' in key and 'ê³¼ëª©' in key]
                    if subject_cols and pd.notna(st.session_state.selected_instructor[subject_cols[0]]):
                        st.markdown(f"**ğŸ“š ê°•ì˜ ê³¼ëª©:** {st.session_state.selected_instructor[subject_cols[0]]}")
                    
                    # ì´ë©”ì¼
                    email_cols = [key for key in st.session_state.selected_instructor.keys() if 'e-mail' in key or 'ì´ë©”ì¼' in key]
                    if email_cols and pd.notna(st.session_state.selected_instructor[email_cols[0]]):
                        st.markdown(f"**ğŸ“§ ì´ë©”ì¼:** {st.session_state.selected_instructor[email_cols[0]]}")
                    
                    # ëŒ€ë¶„ì•¼
                    main_cat_cols = [key for key in st.session_state.selected_instructor.keys() if 'ëŒ€ë¶„ì•¼' in key]
                    if main_cat_cols and pd.notna(st.session_state.selected_instructor[main_cat_cols[0]]):
                        st.markdown(f"**ğŸ·ï¸ ëŒ€ë¶„ì•¼:** {st.session_state.selected_instructor[main_cat_cols[0]]}")
                    
                    # ì†Œë¶„ì•¼
                    sub_cat_cols = [key for key in st.session_state.selected_instructor.keys() if 'ì†Œë¶„ì•¼' in key]
                    if sub_cat_cols and pd.notna(st.session_state.selected_instructor[sub_cat_cols[0]]):
                        st.markdown(f"**ğŸ·ï¸ ì†Œë¶„ì•¼:** {st.session_state.selected_instructor[sub_cat_cols[0]]}")
                    
                    # ë§Œì¡±ë„
                    satisfaction_cols = [key for key in st.session_state.selected_instructor.keys() if 'ë§Œì¡±ë„' in key]
                    if satisfaction_cols and pd.notna(st.session_state.selected_instructor[satisfaction_cols[0]]):
                        st.markdown(f"**â­ ë§Œì¡±ë„:** {st.session_state.selected_instructor[satisfaction_cols[0]]}")
                    
                    # í•™ìŠµì ì£¼ìš” ì˜ê²¬
                    feedback_cols = [key for key in st.session_state.selected_instructor.keys() if 'í•™ìŠµì' in key or 'ì˜ê²¬' in key]
                    if feedback_cols and pd.notna(st.session_state.selected_instructor[feedback_cols[0]]):
                        st.markdown("**ğŸ’¬ í•™ìŠµì ì£¼ìš” ì˜ê²¬:**")
                        st.markdown(f"{st.session_state.selected_instructor[feedback_cols[0]]}")
                    
                    # ë‹´ë‹¹ì ì˜ê²¬
                    manager_cols = [key for key in st.session_state.selected_instructor.keys() if 'ë‹´ë‹¹ì' in key]
                    if manager_cols and pd.notna(st.session_state.selected_instructor[manager_cols[0]]):
                        st.markdown("**ğŸ“ ë‹´ë‹¹ì ì˜ê²¬:**")
                        st.markdown(f"{st.session_state.selected_instructor[manager_cols[0]]}")
                    
                    # ìœ íŠœë¸Œ ë§í¬ ê²€ìƒ‰ ë° í‘œì‹œ
                    instructor_name = None
                    instructor_job = None
                    instructor_main_field = None
                    instructor_sub_field = None
                    
                    if name_cols:
                        instructor_name = st.session_state.selected_instructor[name_cols[0]]
                    
                    # ì§ì—… ì •ë³´ ì¶”ì¶œ
                    if job_cols and pd.notna(st.session_state.selected_instructor.get(job_cols[0])):
                        instructor_job = st.session_state.selected_instructor[job_cols[0]]
                    
                    # ëŒ€ë¶„ì•¼ ì •ë³´ ì¶”ì¶œ
                    if main_cat_cols and pd.notna(st.session_state.selected_instructor.get(main_cat_cols[0])):
                        instructor_main_field = st.session_state.selected_instructor[main_cat_cols[0]]
                    
                    # ì†Œë¶„ì•¼ ì •ë³´ ì¶”ì¶œ
                    if sub_cat_cols and pd.notna(st.session_state.selected_instructor.get(sub_cat_cols[0])):
                        instructor_sub_field = st.session_state.selected_instructor[sub_cat_cols[0]]
                    
                    if instructor_name and pd.notna(instructor_name):
                        # ìœ íŠœë¸Œ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ìºì‹œ (ì¶”ê°€ ì •ë³´ í¬í•¨í•˜ì—¬ ê³ ìœ  í‚¤ ìƒì„±)
                        cache_key_parts = [instructor_name]
                        if instructor_job:
                            cache_key_parts.append(str(instructor_job))
                        if instructor_sub_field:
                            cache_key_parts.append(str(instructor_sub_field))
                        elif instructor_main_field:
                            cache_key_parts.append(str(instructor_main_field))
                        
                        youtube_cache_key = f"youtube_links_{'_'.join(cache_key_parts)}"
                        
                        if youtube_cache_key not in st.session_state:
                            with st.spinner("ìœ íŠœë¸Œ ì±„ë„/ë™ì˜ìƒ ê²€ìƒ‰ ì¤‘..."):
                                youtube_links = search_youtube_channel(
                                    instructor_name, 
                                    job=instructor_job,
                                    main_field=instructor_main_field,
                                    sub_field=instructor_sub_field
                                )
                                st.session_state[youtube_cache_key] = youtube_links
                        else:
                            youtube_links = st.session_state[youtube_cache_key]
                        
                        if youtube_links:
                            # ìœ íŠœë¸Œ ë¦¬ìŠ¤íŠ¸ ë° ìš”ì•½ ì •ë³´ í‘œì‹œ
                            display_youtube_list_and_summary(youtube_links, instructor_name, instructor_name)
                    
                    st.markdown('</div>', unsafe_allow_html=True)
                
                st.markdown("---")

# ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ (rerun í›„ì—ë„ ìœ ì§€)
if st.session_state.web_search_result:
    # ê²€ìƒ‰ì–´ ê°€ì ¸ì˜¤ê¸°
    search_query_for_display = st.session_state.get('last_search_query', search_query)
    
    st.warning(f"'{search_query_for_display}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    if True:  # í•­ìƒ ì›¹ ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
        st.markdown("---")
        st.markdown("### ğŸŒ ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰ ê²°ê³¼")
        
        web_result = st.session_state.web_search_result
        st.markdown('<div class="info-card" style="border-left-color: #03c75a;">', unsafe_allow_html=True)
        
        # ì œëª©
        st.markdown(f"#### ğŸ‘¤ **{web_result.get('name', search_query)}**")
        st.caption(f"ì¶œì²˜: {web_result.get('source', 'ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰')}")
        
        # ì •ë³´ í‘œì‹œ
        if web_result.get('info'):
            info = web_result['info']
            
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ í‘œì‹œ
            if 'ì´ë¯¸ì§€' in info and info['ì´ë¯¸ì§€']:
                st.image(info['ì´ë¯¸ì§€'], width=150)
            
            # ì´ë¦„
            if 'ì´ë¦„' in info:
                st.markdown(f"**ì´ë¦„:** {info['ì´ë¦„']}")
            
            # ê¸°ë³¸ ì •ë³´ (ìƒë…„ì›”ì¼, ì§ì—…, ì†Œì† ë“±)
            for key, value in info.items():
                if key not in ['ì´ë¦„', 'ì´ë¯¸ì§€', 'ì„¤ëª…', 'ì•½ë ¥', 'ìœ íŠœë¸Œ'] and value:
                    st.markdown(f"**{key}:** {value}")
            
            # ì„¤ëª…
            if 'ì„¤ëª…' in info and info['ì„¤ëª…']:
                st.markdown("---")
                st.markdown("**ğŸ“ ì„¤ëª…:**")
                st.markdown(info['ì„¤ëª…'])
            
            # ì•½ë ¥
            if 'ì•½ë ¥' in info and info['ì•½ë ¥']:
                st.markdown("---")
                st.markdown("**ğŸ“š ì•½ë ¥:**")
                st.markdown(info['ì•½ë ¥'])
            
            # ìœ íŠœë¸Œ ë§í¬ ê²€ìƒ‰ ë° í‘œì‹œ (ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì™€ ê´€ê³„ì—†ì´ ìœ íŠœë¸Œì—ì„œ ì§ì ‘ ê²€ìƒ‰)
            person_name = web_result.get('name', search_query)
            if person_name:
                st.markdown("---")
                # ìœ íŠœë¸Œ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ìºì‹œ
                youtube_cache_key = f"youtube_links_naver_{person_name}"
                if youtube_cache_key not in st.session_state:
                    with st.spinner("ìœ íŠœë¸Œ ì±„ë„/ë™ì˜ìƒ ê²€ìƒ‰ ì¤‘..."):
                        youtube_links = search_youtube_channel(person_name)
                        st.session_state[youtube_cache_key] = youtube_links
                else:
                    youtube_links = st.session_state[youtube_cache_key]
                
                if youtube_links:
                    # ìœ íŠœë¸Œ ë¦¬ìŠ¤íŠ¸ ë° ìš”ì•½ ì •ë³´ í‘œì‹œ
                    display_youtube_list_and_summary(youtube_links, person_name, f"naver_{person_name}")
        
        # ë„¤ì´ë²„ ê²€ìƒ‰ ë§í¬
        if web_result.get('url'):
            st.markdown("---")
            st.markdown(f"[ğŸ”— ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰ì—ì„œ ë” ë³´ê¸°]({web_result['url']})")
        
        st.markdown('</div>', unsafe_allow_html=True)

# ê²€ìƒ‰ ë²„íŠ¼ì´ ëˆŒë ¸ì§€ë§Œ ê²°ê³¼ê°€ ì—†ê³  ì›¹ ê²€ìƒ‰ ê²°ê³¼ë„ ì—†ëŠ” ê²½ìš°
elif search_button and search_query and st.session_state.search_results.empty and not st.session_state.web_search_result:
    st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.info("ğŸ’¡ **íŒ:** ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ 'ì „ì²´' ê²€ìƒ‰ ë²”ìœ„ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    st.info("ğŸ’¡ **íŒ:** ê°•ì‚¬ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰í•˜ë©´ ë„¤ì´ë²„ ì¸ë¬¼ê²€ìƒ‰ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ìœ íŠœë¸Œì—ì„œ ê²€ìƒ‰ ì œê³µ
    st.markdown("---")
    st.markdown(f"### ğŸ“º '{search_query}' ìœ íŠœë¸Œ ê²€ìƒ‰ ê²°ê³¼")
    
    # ìœ íŠœë¸Œ ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ìºì‹œ
    youtube_cache_key = f"youtube_links_direct_{search_query}"
    if youtube_cache_key not in st.session_state:
        with st.spinner("ìœ íŠœë¸Œ ì±„ë„/ë™ì˜ìƒ ê²€ìƒ‰ ì¤‘..."):
            youtube_links = search_youtube_channel(search_query)
            st.session_state[youtube_cache_key] = youtube_links
    else:
        youtube_links = st.session_state[youtube_cache_key]
    
    if youtube_links:
        # ìœ íŠœë¸Œ ë¦¬ìŠ¤íŠ¸ ë° ìš”ì•½ ì •ë³´ í‘œì‹œ
        display_youtube_list_and_summary(youtube_links, search_query, f"direct_{search_query}")

# ì‚¬ì´ë“œë°”ì— í†µê³„ í‘œì‹œ
with st.sidebar:
    st.markdown("### ğŸ“Š í†µê³„")
    
    # ì¤‘ë³µ ì œê±°ëœ ê°•ì‚¬ ìˆ˜ ê³„ì‚° (ì´ë¦„ + ì´ë©”ì¼ì´ ê°™ìœ¼ë©´ ë™ì¼ì¸ë¬¼)
    if not df.empty:
        # ì»¬ëŸ¼ëª… ì°¾ê¸°
        name_cols = [col for col in df.columns if 'ê°•ì‚¬' in col and 'ì´ë¦„' in col]
        email_cols = [col for col in df.columns if 'e-mail' in col.lower() or 'ì´ë©”ì¼' in col]
        subfield_cols = [col for col in df.columns if 'ì†Œë¶„ì•¼' in col]
        
        # ì¤‘ë³µ ì œê±°ëœ ê°•ì‚¬ ìˆ˜ (ì´ë¦„ + ì´ë©”ì¼ ê¸°ì¤€)
        if name_cols and email_cols:
            name_col = name_cols[0]
            email_col = email_cols[0]
            # ì´ë¦„ê³¼ ì´ë©”ì¼ì´ ëª¨ë‘ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§í•˜ì—¬ ì¤‘ë³µ ì œê±°
            df_with_info = df[df[name_col].notna() & df[email_col].notna()]
            unique_count = df_with_info.drop_duplicates(subset=[name_col, email_col]).shape[0]
            
            st.metric("ì´ ê°•ì‚¬ ìˆ˜", unique_count)
        else:
            # ì´ë¦„ì´ë‚˜ ì´ë©”ì¼ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ í–‰ ìˆ˜ í‘œì‹œ
            st.metric("ì´ ê°•ì‚¬ ìˆ˜", len(df))
        
        # ì†Œë¶„ì•¼ë³„ ê°•ì‚¬ ìˆ˜ í‘œì‹œ
        if subfield_cols:
            st.markdown("---")
            st.markdown("### ğŸ“ˆ ì†Œë¶„ì•¼ë³„ í†µê³„")
            
            subfield_col = subfield_cols[0]
            
            # ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¡œ ì†Œë¶„ì•¼ë³„ ì§‘ê³„
            if name_cols and email_cols:
                df_unique = df_with_info.drop_duplicates(subset=[name_col, email_col])
            else:
                df_unique = df.copy()
            
            # ì†Œë¶„ì•¼ë³„ ì¹´ìš´íŠ¸
            subfield_counts = df_unique[subfield_col].value_counts()
            
            # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            top_subfields = subfield_counts.head(10)
            
            if not top_subfields.empty:
                # ì§€í‘œë¡œ í‘œì‹œ
                st.markdown("**Top 5 ì†Œë¶„ì•¼:**")
                for idx, (subfield, count) in enumerate(top_subfields.head(5).items(), 1):
                    if pd.notna(subfield) and subfield != '':
                        st.markdown(f"{idx}. **{subfield}**: {count}ëª…")
                
                # ê·¸ë˜í”„ í‘œì‹œ
                st.markdown("---")
                st.markdown("**ì „ì²´ ì†Œë¶„ì•¼ ë¶„í¬:**")
                
                # Streamlit ë‚´ì¥ bar_chart ì‚¬ìš©
                st.bar_chart(top_subfields)
                
                # ìƒì„¸ ì •ë³´
                with st.expander("ğŸ“Š ì „ì²´ ì†Œë¶„ì•¼ ë³´ê¸°"):
                    for subfield, count in subfield_counts.items():
                        if pd.notna(subfield) and subfield != '':
                            st.markdown(f"â€¢ {subfield}: {count}ëª…")
    else:
        st.metric("ì´ ê°•ì‚¬ ìˆ˜", 0)
    
    # ê²€ìƒ‰ ê¸°ë¡ í‘œì‹œ
    if search_query and search_button:
        results = search_instructors(df, search_query, search_type)
        if not results.empty:
            st.markdown("---")
            st.metric("ê²€ìƒ‰ ê²°ê³¼", len(results))

# í‘¸í„°
st.markdown("---")
st.caption("ğŸ’¡ í•œ ê°€ì§€ ê²€ìƒ‰ì–´ë§Œ ì…ë ¥í•´ë„ ê°•ì‚¬ ì´ë¦„, ì†Œì†, ì§ì—…ì´ í‘œì‹œë˜ë©°, í´ë¦­í•˜ë©´ ìƒì„¸ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

